{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## <font color=\"blue\"> Importing Libraries","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import os\nimport json\nimport torch\nimport nltk\n\nimport numpy as np\nimport pandas as pd\nimport torchvision.transforms as transforms\nimport matplotlib.image as mpimg\nimport matplotlib.pyplot as plt\n\nfrom torch import nn\nfrom collections import Counter\nfrom PIL import Image\nfrom string import punctuation\nfrom tqdm import tqdm\nfrom gensim import corpora\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom torch.nn.utils.rnn import pack_padded_sequence\nfrom keras.utils import pad_sequences\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import confusion_matrix\nfrom torchvision.models import resnet101, ResNet101_Weights\nfrom nltk.translate.bleu_score import corpus_bleu, sentence_bleu\n\npd.set_option('display.max_colwidth', None)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T16:04:59.254829Z","iopub.execute_input":"2023-09-23T16:04:59.255191Z","iopub.status.idle":"2023-09-23T16:05:24.641516Z","shell.execute_reply.started":"2023-09-23T16:04:59.255141Z","shell.execute_reply":"2023-09-23T16:05:24.640516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!wget http://nlp.stanford.edu/data/glove.6B.zip\n!unzip glove.6B.zip\n!ls -lat","metadata":{"execution":{"iopub.status.busy":"2023-09-23T16:05:24.643269Z","iopub.execute_input":"2023-09-23T16:05:24.644522Z","iopub.status.idle":"2023-09-23T16:08:27.624491Z","shell.execute_reply.started":"2023-09-23T16:05:24.644484Z","shell.execute_reply":"2023-09-23T16:08:27.623034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font color=\"blue\"> Reading caption and image dir file","metadata":{}},{"cell_type":"code","source":"image_path = \"/kaggle/input/flickr8k/Images/\"\ndf = pd.read_csv(\"/kaggle/input/flickr8k/captions.txt\")\ndf[\"image\"].nunique()","metadata":{"execution":{"iopub.status.busy":"2023-09-23T16:08:39.699954Z","iopub.execute_input":"2023-09-23T16:08:39.700364Z","iopub.status.idle":"2023-09-23T16:08:39.846764Z","shell.execute_reply.started":"2023-09-23T16:08:39.700328Z","shell.execute_reply":"2023-09-23T16:08:39.845662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font color=\"blue\"> Load Pretrained Glove","metadata":{}},{"cell_type":"code","source":"vocab,embeddings = [],[]\nwith open('/kaggle/working/glove.6B.100d.txt','rt') as fi:\n    full_content = fi.read().strip().split('\\n')\nprint(full_content[0])\nfor i in range(len(full_content)):\n    i_word = full_content[i].split(' ')[0]\n    i_embeddings = [float(val) for val in full_content[i].split(' ')[1:]]\n    vocab.append(i_word)\n    embeddings.append(i_embeddings)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T16:08:40.713786Z","iopub.execute_input":"2023-09-23T16:08:40.714131Z","iopub.status.idle":"2023-09-23T16:08:58.822978Z","shell.execute_reply.started":"2023-09-23T16:08:40.714100Z","shell.execute_reply":"2023-09-23T16:08:58.821955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab_npa = np.array(vocab)\nembs_npa = np.array(embeddings)\nvocab_npa = np.insert(vocab_npa, 0, '<pad>')\nvocab_npa = np.insert(vocab_npa, 1, '<unk>')\nvocab_npa = np.insert(vocab_npa, 2, '<start>')\nvocab_npa = np.insert(vocab_npa, 3, '<end>')\npad_emb_npa = np.zeros((1,embs_npa.shape[1]))\nstart_emb_npa = np.random.rand(1,embs_npa.shape[1])\nend_emb_npa = np.random.rand(1,embs_npa.shape[1])\nunk_emb_npa = np.mean(embs_npa,axis=0,keepdims=True)\nembs_npa = np.vstack((pad_emb_npa,unk_emb_npa,start_emb_npa,end_emb_npa,embs_npa))","metadata":{"execution":{"iopub.status.busy":"2023-09-23T16:08:58.824963Z","iopub.execute_input":"2023-09-23T16:08:58.826446Z","iopub.status.idle":"2023-09-23T16:09:04.404267Z","shell.execute_reply.started":"2023-09-23T16:08:58.826408Z","shell.execute_reply":"2023-09-23T16:09:04.403220Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font color=\"blue\"> Check sample images","metadata":{}},{"cell_type":"code","source":"def show_image_and_its_caption(index):\n    image_name, caption = df.iloc[[index]][\"image\"].values[0], df.iloc[[index]][\"caption\"].values[0]\n    img = mpimg.imread(os.path.join(image_path, image_name))\n    plt.imshow(img)\n    print(caption)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T16:09:04.405832Z","iopub.execute_input":"2023-09-23T16:09:04.406208Z","iopub.status.idle":"2023-09-23T16:09:04.411960Z","shell.execute_reply.started":"2023-09-23T16:09:04.406155Z","shell.execute_reply":"2023-09-23T16:09:04.410946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_image_and_its_caption(98)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T16:09:04.414710Z","iopub.execute_input":"2023-09-23T16:09:04.415346Z","iopub.status.idle":"2023-09-23T16:09:04.794992Z","shell.execute_reply.started":"2023-09-23T16:09:04.415303Z","shell.execute_reply":"2023-09-23T16:09:04.794033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font color=\"blue\"> Preprocess caption","metadata":{}},{"cell_type":"code","source":"unique_words = Counter()\ndf[\"tokens\"] = None\nmax_caption_len = 30\nfor index, row in tqdm(df.iterrows(), total=df.shape[0]):\n    words = nltk.word_tokenize(row[\"caption\"])\n    words = [word.lower() for word in words if word.isalpha() and word not in punctuation]\n    if len(words) <= max_caption_len:\n        df.at[index, \"tokens\"] = words\n    else:\n        df = df.drop(index)\n        continue\n    unique_words.update(words)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T16:09:04.796054Z","iopub.execute_input":"2023-09-23T16:09:04.797203Z","iopub.status.idle":"2023-09-23T16:09:17.620389Z","shell.execute_reply.started":"2023-09-23T16:09:04.797151Z","shell.execute_reply":"2023-09-23T16:09:17.619541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font color=\"blue\"> Take more frequent words","metadata":{}},{"cell_type":"code","source":"min_word_freq = 20\nwords = [w for w in unique_words.keys() if unique_words[w] > min_word_freq]\nword_map = {k: v+1 for v, k in enumerate(words)}\nword_map[\"<unk>\"] = len(word_map) + 1\nword_map[\"<start>\"] = len(word_map) + 1\nword_map[\"<end>\"] = len(word_map) + 1\nword_map[\"<pad>\"] = 0\nlen(unique_words), len(words), len(word_map)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T16:09:17.624558Z","iopub.execute_input":"2023-09-23T16:09:17.631353Z","iopub.status.idle":"2023-09-23T16:09:17.649128Z","shell.execute_reply.started":"2023-09-23T16:09:17.631319Z","shell.execute_reply":"2023-09-23T16:09:17.648324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embeddings_focus = []\nkeys = list(word_map.keys())\nvalues = list(word_map.values())\nsorted_value_index = np.argsort(values)\nsorted_word_map = {keys[i]: values[i] for i in sorted_value_index}\nfor key, value in sorted_word_map.items():\n    index = np.where(vocab_npa == key)[0][0]\n    embedding = embs_npa[index]\n    embeddings_focus.append(embedding)\nembeddings_focus = np.array(embeddings_focus)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T16:09:17.653420Z","iopub.execute_input":"2023-09-23T16:09:17.655546Z","iopub.status.idle":"2023-09-23T16:09:30.773217Z","shell.execute_reply.started":"2023-09-23T16:09:17.655512Z","shell.execute_reply":"2023-09-23T16:09:30.772200Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"index_to_word = {}\nfor k, v in sorted_word_map.items():\n    index_to_word[v] = k","metadata":{"execution":{"iopub.status.busy":"2023-09-23T16:09:30.774595Z","iopub.execute_input":"2023-09-23T16:09:30.775653Z","iopub.status.idle":"2023-09-23T16:09:30.780823Z","shell.execute_reply.started":"2023-09-23T16:09:30.775613Z","shell.execute_reply":"2023-09-23T16:09:30.779914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font color=\"blue\"> Encode Captions","metadata":{}},{"cell_type":"code","source":"df[\"caption_encoded\"] = None\ndf[\"caption_length\"] = None\nfor index, row in tqdm(df.iterrows(), total=df.shape[0]):\n    words = row[\"tokens\"]\n    encoded_caption = [word_map[\"<start>\"]] + [word_map.get(word, word_map[\"<unk>\"]) for word in words] + [\n        word_map[\"<end>\"]] + [word_map[\"<pad>\"]]*(max_caption_len - len(words))\n    df.at[index, \"caption_encoded\"] = encoded_caption\n    df.at[index, \"caption_length\"] = len(words) + 2","metadata":{"execution":{"iopub.status.busy":"2023-09-23T16:09:30.782070Z","iopub.execute_input":"2023-09-23T16:09:30.782991Z","iopub.status.idle":"2023-09-23T16:09:35.246539Z","shell.execute_reply.started":"2023-09-23T16:09:30.782957Z","shell.execute_reply":"2023-09-23T16:09:35.244979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_ = df.groupby(\"image\").apply(lambda x: x.sample(n=4) if x.shape[0] >= 4 else None).reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T16:09:35.251115Z","iopub.execute_input":"2023-09-23T16:09:35.251933Z","iopub.status.idle":"2023-09-23T16:09:39.708357Z","shell.execute_reply.started":"2023-09-23T16:09:35.251895Z","shell.execute_reply":"2023-09-23T16:09:39.707347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font color=\"blue\"> Encoder","metadata":{}},{"cell_type":"code","source":"class ImageEncoder(nn.Module):\n    def __init__(self, encoded_image_size=14):\n        super().__init__()\n        self.resnet101 = resnet101(pretrained=True)\n        modules = list(self.resnet101.children())[:-2]\n        self.resnet = nn.Sequential(*modules)\n        self.adaptive_pooling = nn.AdaptiveAvgPool2d((encoded_image_size, encoded_image_size))\n        self.fine_tune()\n    def forward(self, image):\n        x = self.resnet(image)\n        x = self.adaptive_pooling(x)\n        x = x.permute(0,2,3,1)\n        return x\n    def fine_tune(self, fine_tune=True):\n        for p in self.resnet.parameters():\n            p.requires_grad = False\n        for layer in list(self.resnet.children())[5:]:\n            for p in layer.parameters():\n                p.requires_grad = fine_tune","metadata":{"execution":{"iopub.status.busy":"2023-09-23T16:09:39.709993Z","iopub.execute_input":"2023-09-23T16:09:39.710408Z","iopub.status.idle":"2023-09-23T16:09:39.720871Z","shell.execute_reply.started":"2023-09-23T16:09:39.710366Z","shell.execute_reply":"2023-09-23T16:09:39.719856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ie = ImageEncoder(14)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T16:09:39.722316Z","iopub.execute_input":"2023-09-23T16:09:39.722808Z","iopub.status.idle":"2023-09-23T16:09:42.729442Z","shell.execute_reply.started":"2023-09-23T16:09:39.722773Z","shell.execute_reply":"2023-09-23T16:09:42.728436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img = Image.open(\"/kaggle/input/flickr8k/Images/1000268201_693b08cb0e.jpg\")\nnew_image = img.resize((224, 224))\nnp_image = np.array(new_image).astype('float32')/255\nnp_image = np.expand_dims(np_image, axis=0)\ntensor = torch.transpose(torch.from_numpy(np_image), 1, 3)\ntensor.size()","metadata":{"execution":{"iopub.status.busy":"2023-09-23T16:09:42.730967Z","iopub.execute_input":"2023-09-23T16:09:42.731333Z","iopub.status.idle":"2023-09-23T16:09:42.757509Z","shell.execute_reply.started":"2023-09-23T16:09:42.731303Z","shell.execute_reply":"2023-09-23T16:09:42.756547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nie.to(device)\nout = ie(tensor.to(device))\nout.size()","metadata":{"execution":{"iopub.status.busy":"2023-09-23T16:09:42.758866Z","iopub.execute_input":"2023-09-23T16:09:42.759680Z","iopub.status.idle":"2023-09-23T16:09:53.089111Z","shell.execute_reply.started":"2023-09-23T16:09:42.759641Z","shell.execute_reply":"2023-09-23T16:09:53.088118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font color=\"blue\"> Attention Module for encoded image and previous decoder state","metadata":{}},{"cell_type":"code","source":"class Attention(nn.Module):\n    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n        super().__init__()\n        self.encoder_attn = nn.Linear(encoder_dim, attention_dim)\n        self.decoder_attn = nn.Linear(decoder_dim, attention_dim)\n        self.aggregate = nn.Linear(attention_dim, 1)\n        self.relu = nn.ReLU()\n        self.softmax = nn.Softmax()\n    def forward(self, encoder_out, decoder_out):\n        encoder_attn = self.encoder_attn(encoder_out)\n        decoder_attn = self.decoder_attn(decoder_out)\n        attention = self.aggregate(self.relu(encoder_attn + decoder_attn.unsqueeze(1))).squeeze(2)\n        alpha = self.softmax(attention)\n        attention_weighted_encoding = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)\n        return attention_weighted_encoding, alpha","metadata":{"execution":{"iopub.status.busy":"2023-09-23T16:09:53.090475Z","iopub.execute_input":"2023-09-23T16:09:53.090907Z","iopub.status.idle":"2023-09-23T16:09:53.099673Z","shell.execute_reply.started":"2023-09-23T16:09:53.090872Z","shell.execute_reply":"2023-09-23T16:09:53.098775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font color=\"blue\"> Decoder with Attention","metadata":{}},{"cell_type":"code","source":"class DecoderWithAttention(nn.Module):\n    def __init__(self, decoder_dim, attention_dim, embed_dim, vocab_size, dropout=0.5,encoder_dim=2048):\n        super().__init__()\n        self.encoder_dim = encoder_dim\n        self.decoder_dim = decoder_dim\n        self.attention_dim = attention_dim\n        self.embed_dim = embed_dim\n        self.vocab_size = vocab_size\n        self.dropout = dropout\n        \n        self.embedding = nn.Embedding.from_pretrained(torch.from_numpy(embeddings_focus).float())\n        self.attention = Attention(encoder_dim, decoder_dim, attention_dim)\n        self.lstmcell = nn.LSTMCell(embed_dim + encoder_dim, decoder_dim, bias=True)\n        self.classification = nn.Linear(decoder_dim, vocab_size)\n        self.f_gate = nn.Linear(decoder_dim, encoder_dim)\n        self.init_h = nn.Linear(encoder_dim, decoder_dim)\n        self.init_c = nn.Linear(encoder_dim, decoder_dim)\n        self.sigmoid = nn.Sigmoid()\n        self.dropout_layer = nn.Dropout(self.dropout)\n        self.fine_tune_embeddings()\n    \n    def init_h_init_c(self, encoder_out):\n        h = self.init_h(encoder_out).mean(dim=1)\n        c = self.init_c(encoder_out).mean(dim=1)\n        return h, c\n\n    def fine_tune_embeddings(self):\n        for p in self.embedding.parameters():\n            p.requires_grad = True\n        \n    def forward(self, encoder_out, encoded_captions, captions_length):\n        batch_size = encoder_out.size()[0]\n        encoder_out = encoder_out.view(encoder_out.size()[0], -1, encoder_out.size()[-1])\n        captions_length, sort_ind = captions_length.squeeze(1).sort(dim=0, descending=True)\n        encoded_captions = encoded_captions[sort_ind]\n        encoder_out = encoder_out[sort_ind]\n        embeddings = self.embedding(encoded_captions)\n        h, c = self.init_h_init_c(encoder_out)\n        decoder_sequence_length = (captions_length - 1).tolist()\n        num_pixels = encoder_out.size()[1]\n        predictions = torch.zeros(batch_size, max(decoder_sequence_length), self.vocab_size).to(device)\n        images = torch.zeros(batch_size, max(decoder_sequence_length), num_pixels).to(device)\n        \n        for i in range(max(decoder_sequence_length)):\n            batch_size_i = sum([l > i for l in decoder_sequence_length])\n            attention_weighted_encoding, alpha = self.attention(encoder_out[:batch_size_i], h[:batch_size_i])\n            gate = self.sigmoid(self.f_gate(h[:batch_size_i]))\n            attention_weighted_encoding = gate * attention_weighted_encoding\n            h, c = self.lstmcell(torch.cat([embeddings[:batch_size_i, i, :],attention_weighted_encoding], dim=1),\n                                 (h[:batch_size_i], c[:batch_size_i]))\n            preds = self.classification(self.dropout_layer(h))\n            predictions[:batch_size_i, i, :] = preds\n            images[:batch_size_i, i, :] = alpha\n        return predictions, images, sort_ind, encoded_captions, decoder_sequence_length\n            ","metadata":{"execution":{"iopub.status.busy":"2023-09-23T16:09:53.101231Z","iopub.execute_input":"2023-09-23T16:09:53.101912Z","iopub.status.idle":"2023-09-23T16:09:53.134847Z","shell.execute_reply.started":"2023-09-23T16:09:53.101874Z","shell.execute_reply":"2023-09-23T16:09:53.133874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font color=\"blue\"> DataLoader","metadata":{}},{"cell_type":"code","source":"class CaptionDataLoader(Dataset):\n    def __init__(self, df, image_path, dataset=\"TRAIN\", transform=None):\n        super().__init__()\n        self.image_ids = df[\"image\"].to_list()\n        self.caption_encoded = df[\"caption_encoded\"].to_list()\n        self.caption_lengths = df[\"caption_length\"].to_list()\n        self.image_path = image_path\n        self.dataset = dataset\n        self.transform = transform\n    def __len__(self):\n        return len(self.image_ids)\n    def __getitem__(self, index):\n        img = Image.open(os.path.join(self.image_path, self.image_ids[index]))\n        new_image = img.resize((224, 224))\n        np_image = np.array(new_image).astype('float32')/255\n        np_image = np.expand_dims(np_image, axis=0)\n        image_tensor = torch.transpose(torch.from_numpy(np_image), 1, 3)\n        image_tensor = image_tensor.squeeze(0)\n        if self.transform is not None:\n            image_tensor = self.transform(image_tensor)\n        encoded_captions = torch.tensor(self.caption_encoded[index])\n        captions_length = torch.tensor(np.array([self.caption_lengths[index]]))\n        if self.dataset == \"TRAIN\":\n            return image_tensor, encoded_captions, captions_length\n        else:\n            all_indices = np.where(np.array(self.image_ids) == self.image_ids[index])[0]\n            all_captions = torch.tensor(np.take(self.caption_encoded, all_indices, axis=0))\n            return image_tensor, encoded_captions, captions_length, all_captions","metadata":{"execution":{"iopub.status.busy":"2023-09-23T16:09:53.136213Z","iopub.execute_input":"2023-09-23T16:09:53.136542Z","iopub.status.idle":"2023-09-23T16:09:53.152694Z","shell.execute_reply.started":"2023-09-23T16:09:53.136510Z","shell.execute_reply":"2023-09-23T16:09:53.151725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font color=\"blue\"> Split data into train, val, test","metadata":{}},{"cell_type":"code","source":"image_ids = df_[\"image\"].unique()\ntrain, validate, test = np.split(image_ids, [int(.6*image_ids.shape[0]), int(.65*image_ids.shape[0])])","metadata":{"execution":{"iopub.status.busy":"2023-09-23T16:09:53.154916Z","iopub.execute_input":"2023-09-23T16:09:53.155959Z","iopub.status.idle":"2023-09-23T16:09:53.174636Z","shell.execute_reply.started":"2023-09-23T16:09:53.155926Z","shell.execute_reply":"2023-09-23T16:09:53.173758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = df_[df_[\"image\"].isin(train)]\ndf_validate = df_[df_[\"image\"].isin(validate)]\ndf_test = df_[df_[\"image\"].isin(test)]","metadata":{"execution":{"iopub.status.busy":"2023-09-23T16:09:53.175988Z","iopub.execute_input":"2023-09-23T16:09:53.176341Z","iopub.status.idle":"2023-09-23T16:09:53.198747Z","shell.execute_reply.started":"2023-09-23T16:09:53.176306Z","shell.execute_reply":"2023-09-23T16:09:53.197894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font color=\"blue\"> Check Data Loader","metadata":{}},{"cell_type":"code","source":"normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                     std=[0.229, 0.224, 0.225])\ntrain_loader = CaptionDataLoader(df_train, image_path, transform=transforms.Compose([normalize]))\ntrain_data_loader = DataLoader(train_loader, batch_size=32, num_workers=4, shuffle=True)\nvalid_loader = CaptionDataLoader(df_validate, image_path, dataset=\"VAL\", transform=transforms.Compose([normalize]))\nvalid_data_loader = DataLoader(valid_loader, batch_size=32, num_workers=4, shuffle=True)\nfor image, caption, length, all_caps in valid_data_loader:\n    print(image.size())\n    print(caption.size())\n    print(length.size())\n    print(all_caps.size())\n    break","metadata":{"execution":{"iopub.status.busy":"2023-09-23T16:09:53.200893Z","iopub.execute_input":"2023-09-23T16:09:53.201541Z","iopub.status.idle":"2023-09-23T16:09:56.429509Z","shell.execute_reply.started":"2023-09-23T16:09:53.201505Z","shell.execute_reply":"2023-09-23T16:09:56.428334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AverageMeter:\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.sum += val\n        self.count += n\n        self.avg = self.sum / self.count\n\ndef accuracy(scores, targets, k):\n    batch_size = targets.size(0)\n    _, ind = scores.topk(k, 1, True, True)\n    correct = ind.eq(targets.view(-1, 1).expand_as(ind))\n    correct_total = correct.view(-1).float().sum() \n    return correct_total.item()\ndef clip_gradient(optimizer, grad_clip):\n    for group in optimizer.param_groups:\n        for param in group['params']:\n            if param.grad is not None:\n                param.grad.data.clamp_(-grad_clip, grad_clip)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T16:09:56.431470Z","iopub.execute_input":"2023-09-23T16:09:56.431876Z","iopub.status.idle":"2023-09-23T16:09:56.445667Z","shell.execute_reply.started":"2023-09-23T16:09:56.431837Z","shell.execute_reply":"2023-09-23T16:09:56.444784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font color=\"blue\"> Training the model","metadata":{}},{"cell_type":"code","source":"embed_dim = 100\nattention_dim = 512\ndecoder_dim = 512\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nstart_epoch = 0\nnum_epochs = 8\nencoder_lr = 1e-4\ndecoder_lr = 5e-4\nfine_tune_encoder = True\ngrad_clip = 5\n\ndecoder = DecoderWithAttention(\n    decoder_dim=decoder_dim, embed_dim=embed_dim, vocab_size=len(word_map), attention_dim=attention_dim)\n\ndecoder_optim = torch.optim.Adam(params=filter(lambda p: p.requires_grad, decoder.parameters()), lr=decoder_lr)\n\nencoder = ImageEncoder()\nencoder.fine_tune()\nencoder_optim = torch.optim.Adam(params=filter(lambda p: p.requires_grad, encoder.parameters()), lr=encoder_lr)\n\ndecoder.to(device)\nencoder.to(device)\ncriterion = torch.nn.CrossEntropyLoss().cuda()\nbest_loss = np.inf\nfor i in range(num_epochs):\n    encoder.train()\n    decoder.train()\n    train_losses = AverageMeter()\n    train_top5accs = AverageMeter()\n    for image, captions, caption_length in tqdm(train_data_loader, total=len(train_data_loader)):\n        image = image.to(device)\n        captions = captions.to(device)\n        caption_length = caption_length.to(device)\n        \n        encoder_out = encoder(image)\n        predictions, images, sort_ind, encoded_captions, decoder_sequence_length = decoder(encoder_out, captions, caption_length)\n        \n        targets = encoded_captions[:, 1:]\n        \n        scores, _, _, _ = pack_padded_sequence(predictions, decoder_sequence_length, batch_first=True)\n        targets, _, _, _ = pack_padded_sequence(targets, decoder_sequence_length, batch_first=True)\n        \n        loss = criterion(scores, targets)\n        train_losses.update(loss, sum(decoder_sequence_length))\n        top5 = accuracy(scores, targets, 5)\n        train_top5accs.update(top5, sum(decoder_sequence_length))\n        decoder_optim.zero_grad()\n        encoder_optim.zero_grad()\n        loss.backward()\n        clip_gradient(decoder_optim, grad_clip)\n        clip_gradient(encoder_optim, grad_clip)\n        decoder_optim.step()\n        encoder_optim.step()\n    \n    with torch.no_grad():\n        encoder.eval()\n        decoder.eval()\n        losses = AverageMeter()\n        top5accs = AverageMeter()\n        all_true_captions = list()\n        predicted_caption = list()\n        for image, captions, caption_length, all_captions in tqdm(valid_data_loader, total=len(valid_data_loader)):\n            image = image.to(device)\n            captions = captions.to(device)\n            caption_length = caption_length.to(device)\n            all_captions = all_captions.to(device)\n            encoder_out = encoder(image)\n            predictions, images, sort_ind, encoded_captions, decoder_sequence_length = decoder(encoder_out, captions, caption_length)\n            targets = encoded_captions[:, 1:]\n            scores_copy = predictions.clone()\n            scores, _, _, _ = pack_padded_sequence(predictions, decoder_sequence_length, batch_first=True)\n            targets, _, _, _ = pack_padded_sequence(targets, decoder_sequence_length, batch_first=True)\n            loss = criterion(scores, targets)\n            losses.update(loss, sum(decoder_sequence_length))\n            top5 = accuracy(scores, targets, 5)\n            top5accs.update(top5, sum(decoder_sequence_length))\n#             all_captions = all_captions[sort_ind]\n#             all_captions = all_captions.tolist()\n#             for caps in all_captions:\n#                 cur_caps = list(map(lambda c: [w for w in c if w not in [word_map[\"<start>\"], word_map[\"<pad>\"]]], caps))\n#                 all_true_captions.append(cur_caps)\n#             _, preds = torch.max(scores_copy, dim=2)\n#             preds = preds.tolist()\n#             temp_preds = []\n#             for j in range(len(preds)):\n#                 cur_preds = preds[j][:decoder_sequence_length[j]]\n#                 temp_preds.append(cur_preds)\n#             predicted_caption.extend(temp_preds)\n            \n#             assert len(all_true_captions) == len(predicted_caption)\n#         bleu_score = corpus_bleu(all_true_captions, predicted_caption, weights=(0.1, 0.2, 0.3, 0.4))\n        if losses.val < best_loss:\n            best_loss = losses.val\n            state_dict_encoder = encoder.state_dict()\n            state_dict_decoder = decoder.state_dict()\n            \n    print(f\"epoch {i+1}, Train loss = {round(float(train_losses.avg), 4)}, Train Top5 Accuracy = {round(float(train_top5accs.avg), 4)}, Valid loss = {round(float(losses.avg), 4)}, Valid top5 accuracy = {round(float(top5accs.avg), 4)}\")","metadata":{"execution":{"iopub.status.busy":"2023-09-23T16:09:56.447342Z","iopub.execute_input":"2023-09-23T16:09:56.447698Z","iopub.status.idle":"2023-09-23T16:45:35.055778Z","shell.execute_reply.started":"2023-09-23T16:09:56.447664Z","shell.execute_reply":"2023-09-23T16:45:35.054552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font color=\"blue\"> Evaluation on test set","metadata":{}},{"cell_type":"code","source":"torch.save(state_dict_encoder, \"/kaggle/working/encoder.pth\")\ntorch.save(state_dict_decoder, \"/kaggle/working/decoder.pth\")","metadata":{"execution":{"iopub.status.busy":"2023-09-23T16:45:39.819041Z","iopub.execute_input":"2023-09-23T16:45:39.820024Z","iopub.status.idle":"2023-09-23T16:45:40.216199Z","shell.execute_reply.started":"2023-09-23T16:45:39.819985Z","shell.execute_reply":"2023-09-23T16:45:40.215103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoder.load_state_dict(state_dict_encoder)\ndecoder.load_state_dict(state_dict_decoder)\nencoder.to(device)\ndecoder.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T16:45:40.784422Z","iopub.execute_input":"2023-09-23T16:45:40.784792Z","iopub.status.idle":"2023-09-23T16:45:40.844990Z","shell.execute_reply.started":"2023-09-23T16:45:40.784760Z","shell.execute_reply":"2023-09-23T16:45:40.843999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_loader = CaptionDataLoader(df_test, dataset=\"TEST\", image_path=image_path)\ntest_data_loader = DataLoader(test_loader, batch_size=1, num_workers=1)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T16:45:44.448243Z","iopub.execute_input":"2023-09-23T16:45:44.448625Z","iopub.status.idle":"2023-09-23T16:45:44.455810Z","shell.execute_reply.started":"2023-09-23T16:45:44.448593Z","shell.execute_reply":"2023-09-23T16:45:44.454849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"beam_size = 5\nbleu_score = []\nfor image, caps, caplens, allcaps in tqdm(test_data_loader, total=len(test_data_loader)):\n    k = beam_size\n    image = image.to(device)\n    encoder_out = encoder(image)\n    feature_size = encoder_out.size(3)\n    encoder_out = encoder_out.view(1, -1, feature_size)\n    num_pixels = encoder_out.size(1)\n    encoder_out = encoder_out.expand(k, num_pixels, feature_size)\n    k_prev_words = torch.LongTensor([[word_map[\"<start>\"]]] * k).to(device)\n    seqs = k_prev_words\n    topk_scores = torch.zeros(k,1).to(device)\n    complete_sequence = list()\n    complete_sequence_scores = list()\n    h, c = decoder.init_h_init_c(encoder_out)\n    step = 1\n    references = []\n    hypotheses = []\n    while True:\n        embeddings = decoder.embedding(k_prev_words).squeeze(1)\n        attention_encoding, alpha = decoder.attention(encoder_out, h)\n        gate = decoder.sigmoid(decoder.f_gate(h))\n        attention_encoding = gate * attention_encoding\n        h, c = decoder.lstmcell(torch.cat([embeddings, attention_encoding], dim=1), (h,c))\n        output = decoder.classification(h)\n        scores = nn.LogSoftmax(dim=1)(output)\n        scores = topk_scores.expand_as(scores) + scores\n        if step == 1:\n            topk_scores, topk_words_ind = scores[0].topk(k, 0, True, True)\n        else:\n            topk_scores, topk_words_ind = scores.view(-1).topk(k, 0, True, True)\n        prev_word_inds = topk_words_ind // len(word_map)\n        next_word_inds = topk_words_ind % len(word_map)\n        seqs = torch.cat([seqs[prev_word_inds], next_word_inds.unsqueeze(1)], dim = 1)\n        incomplete_ind = [ind for ind, next_word in enumerate(next_word_inds) if next_word != word_map[\"<end>\"]]\n        complete_inds = list(set(range(len(next_word_inds))) - set(incomplete_ind))\n        if len(complete_inds) > 0:\n            k -= len(complete_inds)\n            complete_sequence.extend(seqs[complete_inds].tolist())\n            complete_sequence_scores.extend(topk_scores[complete_inds])\n        \n        seqs = seqs[incomplete_ind]\n        h = h[prev_word_inds[incomplete_ind]]\n        c = c[prev_word_inds[incomplete_ind]]\n        encoder_out = encoder_out[prev_word_inds[incomplete_ind]]\n        k_prev_words = next_word_inds[incomplete_ind].unsqueeze(1)\n        topk_scores = topk_scores[incomplete_ind].unsqueeze(1)\n        if step > 50:\n            break\n        step += 1\n    i = complete_sequence_scores.index(max(complete_sequence_scores))\n    seq = complete_sequence[i]\n\n    img_caps = allcaps[0].tolist()\n    img_captions = list(\n        map(lambda c: [w for w in c if w not in {word_map['<start>'], word_map['<end>'], word_map['<pad>']}],\n            img_caps))  # remove <start> and pads\n    references.append(img_captions)\n\n    # Hypotheses\n    hypotheses.append([w for w in seq if w not in {word_map['<start>'], word_map['<end>'], word_map['<pad>']}])\n    bleu4 = 0\n    bleu4 = sentence_bleu(references[0], hypotheses[0])\n    bleu_score.append(bleu4)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T16:49:57.393938Z","iopub.execute_input":"2023-09-23T16:49:57.394369Z","iopub.status.idle":"2023-09-23T17:08:07.771530Z","shell.execute_reply.started":"2023-09-23T16:49:57.394332Z","shell.execute_reply":"2023-09-23T17:08:07.770448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sum(bleu_score)/len(bleu_score)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T17:08:09.756327Z","iopub.execute_input":"2023-09-23T17:08:09.756694Z","iopub.status.idle":"2023-09-23T17:08:09.764134Z","shell.execute_reply.started":"2023-09-23T17:08:09.756663Z","shell.execute_reply":"2023-09-23T17:08:09.763192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font color=\"blue\"> Generate Caption from image. Enter image path as input","metadata":{}},{"cell_type":"code","source":"def generate_caption(image_path):\n    beam_size = 3\n    k = beam_size\n    img = Image.open(image_path)\n    new_image = img.resize((224, 224))\n    plt.imshow(new_image)\n    np_image = np.array(new_image).astype('float32')/255\n    np_image = np.expand_dims(np_image, axis=0)\n    image_tensor = torch.transpose(torch.from_numpy(np_image), 1, 3)\n    image_tensor = image_tensor.to(device)\n    encoder_out = encoder(image_tensor)\n    feature_size = encoder_out.size(3)\n    encoder_out = encoder_out.view(1, -1, feature_size)\n    num_pixels = encoder_out.size(1)\n    encoder_out = encoder_out.expand(k, num_pixels, feature_size)\n    k_prev_words = torch.LongTensor([[word_map[\"<start>\"]]] * k).to(device)\n    seqs = k_prev_words\n    topk_scores = torch.zeros(k,1).to(device)\n    complete_sequence = list()\n    complete_sequence_scores = list()\n    h, c = decoder.init_h_init_c(encoder_out)\n    step = 1\n    hypothesis = []\n    while True:\n        embeddings = decoder.embedding(k_prev_words).squeeze(1)\n        attention_encoding, alpha = decoder.attention(encoder_out, h)\n        gate = decoder.sigmoid(decoder.f_gate(h))\n        attention_encoding = gate * attention_encoding\n        h, c = decoder.lstmcell(torch.cat([embeddings, attention_encoding], dim=1), (h,c))\n        output = decoder.classification(h)\n        scores = nn.LogSoftmax(dim=1)(output)\n        scores = topk_scores.expand_as(scores) + scores\n        if step == 1:\n            topk_scores, topk_words_ind = scores[0].topk(k, 0, True, True)\n        else:\n            topk_scores, topk_words_ind = scores.view(-1).topk(k, 0, True, True)\n        prev_word_inds = topk_words_ind // len(word_map)\n        next_word_inds = topk_words_ind % len(word_map)\n        seqs = torch.cat([seqs[prev_word_inds], next_word_inds.unsqueeze(1)], dim = 1)\n        incomplete_ind = [ind for ind, next_word in enumerate(next_word_inds) if next_word != word_map[\"<end>\"]]\n        complete_inds = list(set(range(len(next_word_inds))) - set(incomplete_ind))\n        if len(complete_inds) > 0:\n            k -= len(complete_inds)\n            complete_sequence.extend(seqs[complete_inds].tolist())\n            complete_sequence_scores.extend(topk_scores[complete_inds])\n        if k == 0:\n            break\n        \n        seqs = seqs[incomplete_ind]\n        h = h[prev_word_inds[incomplete_ind]]\n        c = c[prev_word_inds[incomplete_ind]]\n        encoder_out = encoder_out[prev_word_inds[incomplete_ind]]\n        k_prev_words = next_word_inds[incomplete_ind].unsqueeze(1)\n        topk_scores = topk_scores[incomplete_ind].unsqueeze(1)\n        if step > 50:\n            break\n        step += 1\n#     print(complete_sequence_scores)\n#     print(complete_sequence)\n    i = complete_sequence_scores.index(max(complete_sequence_scores))\n    seq = complete_sequence[i]\n#     print(seq)\n    hypothesis.append([w for w in seq if w not in {word_map['<start>'], word_map['<end>'], word_map['<pad>']}])\n    caption = \"\"\n    for index in hypothesis[0]:\n        caption += index_to_word[index] + \" \"\n    plt.imshow(img)\n    print(caption)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T17:08:26.690613Z","iopub.execute_input":"2023-09-23T17:08:26.691124Z","iopub.status.idle":"2023-09-23T17:08:26.709825Z","shell.execute_reply.started":"2023-09-23T17:08:26.691087Z","shell.execute_reply":"2023-09-23T17:08:26.708729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generate_caption(\"/kaggle/input/flickr8k/Images/1020651753_06077ec457.jpg\")","metadata":{"execution":{"iopub.status.busy":"2023-09-23T17:11:22.763526Z","iopub.execute_input":"2023-09-23T17:11:22.763919Z","iopub.status.idle":"2023-09-23T17:11:23.186190Z","shell.execute_reply.started":"2023-09-23T17:11:22.763884Z","shell.execute_reply":"2023-09-23T17:11:23.185377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}